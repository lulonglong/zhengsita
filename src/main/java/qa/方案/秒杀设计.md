## 一、简述

​		说起秒杀系统啊，我相信你一定听过，甚至亲身参与过秒杀活动。作为技术人，你应该也好奇和思考过秒杀系统背后的实现方案，或者你在面试的时候被问到过，毕竟说它是高频考点一点不夸张。

​		秒杀系统五脏俱全，其中所使用的一些技术，方案，以及方案中的方法论思路，都具有一定的普适性，在我们其他项目中也能使用。

​		所以我们挑选出秒杀系统，来作为研究互联网高负载高可用性的技术方案。

![img](https://static001.geekbang.org/resource/image/0b/68/0bbe71ceab7c344cyydfc81bb32e8968.jpg?wh=1973x1502)



## 二、秒杀系统的业务挑战和设计原则



秒杀是电商平台大促狂欢时非常重要的手段之一，用具有价格优势的稀缺商品，来增加电商平台的关注度，带来空前的流量。**因此，秒杀的主要挑战在于**：

- 高并发产生的巨大瞬时流量。秒杀活动的特点，就是将用户全部集中到同一个时刻，然后一起开抢某个热门商品，而热门商品的库存往往又非常少，因此聚集效应产生了巨大的瞬时流量。
- 高并发无法避开的热点数据问题。秒杀活动大家抢购的都是同一个商品，所以这个商品直接就被推到了热点的位置。
- 来自黑产的刷子流量。刷子高频次的请求，会挤占正常用户的抢购通道，也获得了更高的秒杀成功率。这不仅破坏了公平的抢购环境，也给系统服务带来了巨大的额外负担。



从技术层面上来说，我们需要了解 HTTP 服务的请求链路路径，即请求从 DNS 到 Nginx，Nginx 处理后分发给 Web 服务，Web 服务再聚合调用下游的 RPC 服务。

每个链路节点都有它擅长做的事情。只有清楚了这些，我们才能够将秒杀系统提供的业务功能，按不同阶段、不同响应，合理地拆分到不同的链路层级来实现，以符合我们**校验前置、分层过滤、缩短链路的设计原则**，并能够从容应对秒杀系统所面临的瞬时大流量、热点数据、黄牛刷子等各种挑战。





## 三、秒杀系统架构设计

大家先看一下普通系统架构、以秒杀系统的架构差异



![img](https://static001.geekbang.org/resource/image/75/10/75c66bf2cb65bdbb125b06765d148c10.jpg?wh=1800x1575)

​																														常用的系统架构



![img](https://static001.geekbang.org/resource/image/6e/8e/6eaca4f202c11ca5fc4954512cda858e.jpg?wh=1800x1671)

​																												秒杀系统的架构

首先新系统我们依然保留了 HTTP 服务常用的层级调用关系，即 Nginx->Web 服务 ->RPC 服务，这也是绝大部分公司都会使用的一种系统结构。



其次将原先由 Web 服务提供的静态资源放到了 CDN（CDN 是全国都有的服务器，客户端可以根据所处位置自动就近从 CDN 上拉取静态资源，速度更快），来大大减轻抢购瞬时秒杀域名的负担。



最后，同时也是我们所做的最大改变，就是将 Nginx 的职责放大，前置用来做 Web 网关，承担部分业务逻辑校验，并且增加黑白名单、限流和流控的功能，这其实也是考虑到我们的秒杀业务特点所做的调整。这种在 Nginx 里写业务的做法在很多大公司里都是很常见的，像京东是用来做商详、秒杀的业务网关，美团用来做负载均衡接入层，12306 用来做车票查询等等，他们的共同特点都是要面对高并发的业务场景，这也说明在这种业务场景下，我们的设计是得到了真实实践和广泛认可的。



而这么做的目的，就是要充分利用 Nginx 的高并发、高吞吐能力，并且非常契合我们秒杀业务的特点，即入口流量大。但流量组成却非常的混杂，这些请求中，一部分是刷子请求，一部分是无效请求（传参等异常），剩下的才是正常请求，这个的比例可能是 6：1：3，所以需要我们在网关层尽可能多地接收流量进来，并做精确地筛选，将真正有效的 3 成请求分发到下游，剩余的 7 成拦截在网关层。不然把这些流量都打到 Web 服务层，Web 服务再新起线程来处理刷子和无效请求，这是种资源的浪费。所以网关层对秒杀系统而言，至关重要，而 Nginx 刚好可以胜任此项任务。由此可见，Nginx 在我们的系统设计中，扮演着非常重要的角色。

Nginx层主要主要负责：

- 流量筛选：根据黑白名单、登录态和参数有效性等来筛选流量。

- 流量分发：通过设置的负载均衡算法进行流量分发，也可以自定义算法，比如根据 IP 做 hash，或者根据用户 ID 做 hash 等。

- 简单业务以及校验：提供活动数据、活动有效性校验、库存数量校验和其他业务相关的简单校验等。

- 限流：根据 IP 或者自定义关键入参做限流。
- 异常提示页面：主要是进结算页失败的提示页，可能是被限流，被业务校验拦截或者是后端服务异常等。





## 四、秒杀的隔离策略

很自然，为了不让 0.001% 的爆品影响 99.999% 普通商品的交易，我们很快就想到了隔离，隔离是控制危险范围的最直接的手段。而面对超预期的瞬时流量，我们也要采取很多措施进行流量的隔离，防止秒杀流量串访到普通商品交易流程上，带来不可预估的灾难性后果。



![img](https://static001.geekbang.org/resource/image/d6/0b/d6d8e2e1f96c6e037c0b9bf6568d0c0b.jpg?wh=2351x1148)

### 业务隔离

秒杀商品的稀缺性，决定了业务不会像普通商品那样进行投放售卖。一般会有计划地进行营销策划，制订详细的方案，以达到预期的目标。



因此，从业务上看，它是和普通商品完全不一样的售卖流程，它需要一个提**报过程**。大部分的电商平台，会有一个专门的提报系统（提报系统的建设不是秒杀的核心部分，这里不再赘述），商家或者业务可以根据自己的运营计划在提报系统里进行活动提报，提供参与秒杀的商品编号、活动起止时间、库存量、限购规则、风控规则以及参与活动群体的地域分布、预计人数、会员级别等基本信息。



你别小看这个提报过程和这些基本信息，有了这些信息作为输入，我们就能预估出大致的流量、并发数等，并结合系统当前能支撑的容量情况，评估是否需要扩容，是否需要降级或者调整限流策略等，因此业务隔离的重要性可见一斑。



### 系统隔离

接下来我们看下系统隔离。前面已经介绍过商品交易流程大概会用到哪些系统，理论上讲，需要把交易链路上涉及到的系统都单独复制部署一套，隔离干净，但这样做成本比较高，一般大点的电商平台都采用分布式微服务的部署架构，服务数量少则几十个，多则几百个，全部复制一套进行隔离不现实。

所以比较常见的实践是对会被流量冲击比较大的核心系统进行**物理隔离**，而相对链路末端的一些系统，经过前面的削峰之后，流量比较可控了，这些系统就可以不做物理隔离。

![img](https://static001.geekbang.org/resource/image/cd/d5/cd62700f46e2a5yye40637594df388d5.jpg?wh=1250x1208)

我们知道，用户的秒杀习惯，一般是打开商品详情页进行倒计时等待，时间到了点击秒杀按钮进行抢购。因此第一个需要关注的系统就是商品详情页，我们需要申请独立的秒杀详情页域名，独立的 Nginx 负载均衡器，以及独立的详情页后端服务，并采用 Dubbo 独立分组的方式单独提供秒杀服务。



```java
<!-- 暴露你的服务地址 -->
<dubbo:service 
    ref="productService" 
    interface="com.ecommerce.product.service.IProductService"
    protocol="dubbo"
    <!-- 秒杀流程，商品后端微服务的逻辑分组 -->
    group="seckill"
/>
```



```java
<!-- 指定web服务名字 -->
<dubbo:application name="ProductGroup"/>
<!-- 声明服务注册中心 -->
<dubbo:registry protocol="zookeeper" address="127.0.0.1:2181"/>

<!-- 指定传输层通信协议 -->
<dubbo:protocol name="dubbo" port="20881"/>

<!-- 引用你的服务地址 -->
<dubbo:reference 
    id="productService" 
    interface="com.ecommerce.product.service.IProductService"
    protocol="dubbo"
    <!-- 这里引用的分组名，选择了秒杀流程的逻辑分组 -->
    group="seckill"
/>
```

紧接着我们仍需要对域名进行隔离，我们可以向运维部门申请一个独立的域名，专门用来承接秒杀流量，流量从专有域名进来之后，分配到专有的负载均衡器，再路由到专门的微服务分组，这样就做到了应用服务层面从入口到微服务的流量隔离。

![img](https://static001.geekbang.org/resource/image/d9/6f/d9d69c7decae541883f55a030476bf6f.jpg?wh=5540x2840)

以上就是商品详情页的系统隔离方法，交易流程的其他系统，比如结算页、价格中心、订单中心等也可以参照类似的方式进行隔离，这里就不重复讲解了。

在我看来，这里流量冲击比较大的核心系统就是秒杀详情页、秒杀结算页，是需要我们重点关注的对象，而相对链路末端的一些系统，经过前面的削峰之后，流量比较可控了，如接单系统、收银台、支付系统，物理隔离的意义就不大，反而会增加成本。你自己设计秒杀系统的时候，可以格外注意一下。



### 数据隔离

现在，我们已经完成了应用层的隔离。接下来，在数据层面，我们也应该进行相应的隔离，否则如果共用缓存或者共用数据库，一旦瞬时流量把它们冲垮，照样会影响无辜商品的交易。

数据层的专有部署，需要结合秒杀的场景来设计部署拓扑结构，比如 Redis 缓存，一般的场景一主一从就够了，但是在秒杀场景，需要**一主多从**来扛读热点数据。关于热点数据的处理在后面的课程中我们会详细进行介绍，这里先留个悬念。



到这里为止，我们基本学习完了秒杀隔离策略。现在，我们回过头来思考系统隔离中遗留的问题：**怎么让秒杀流量正确地路由到我们隔离出来的专有环境里来呢？**

答案就是对商品进行打标，在商品的主数据上有了秒杀标，那么我们在任何一个环节都可以把这个染色过的流量进行正确地路由了。

那么既然提到了商品打标，这里我再简单介绍一下商品打标的设计思路。当然，电商平台的商品系统设计远远比这复杂。

<u>打标就是一个标记，我们可以使用一个 long 型字段 skuTags 来保存，long 是 64 位，每一位代表一种类型的活动，0 代表否，1 代表是，通过对 skuTags 进行二进制操作即可完成商品的打标和去标。假设秒杀的标识我们定义在 skuTags 的第 11 位，那么要给一个 sku 打上秒杀标，我们就可以对这个标实际进行“或”操作：skuTags=skuTags|1024，这样 skuTags 字段的第 11 位就变成了 1，对其它 bit 位没影响。去标过程相反，同样进行位操作，skuTags=skuTags&~1024，把第 11 位置为 0。</u>



好了，最后我们把整个隔离流程再串一下。首先业务通过提报系统对秒杀 sku 进行提报，系统对秒杀 sku 进行打标，从活动页、列表页或者搜索页点击商品的时候，系统就能识别出秒杀标，路由到秒杀的商品详情页域名，进而进入到专有 Nginx。

然后就是到专有的微服务分组，以及专有的 Redis 缓存了。这里提一下，上面介绍的流量分流实际上是从活动页就开始的，为了节约成本，我们也可以设计在商品详情页进行分流，这样做的好处是商品详情页是通用的实现，也是通用的部署，当用户在详情页点击购买的时候，才根据是否有秒杀标识进行流量分流。劣势就是进行秒杀的时候，商品详情页的流量压力会比较大。



**小结**

**秒杀系统的特点倒逼我们不得不做流量隔离。如果不做隔离，任由流量互相横冲直撞，将会对电商平台造成很大的影响。隔离的措施概括下来有三种：业务隔离、系统隔离和数据隔离**。



## 五、秒杀的流量管控

如何有效地管控流量？

通过对秒杀流量的隔离，我们已经能够把巨大瞬时流量的影响范围控制在隔离的秒杀环境里了。接下来，我们开始考虑隔离环境的高可用问题，通俗点说，普通商品交易流程保住了，现在就要看怎么把秒杀系统搞稳定，来应对流量冲击，让秒杀系统也不出问题。方法很多，**有流量控制、削峰、限流、缓存热点处理、扩容、熔断**等一系列措施。

在库存有限的情况下，过多的用户参与实际上对电商平台的价值是边际递减的。举个例子，1 万的茅台库存，100 万用户进来秒杀和 1000 万用户进来秒杀，对电商平台而言，所带来的经济效益、社会影响不会有 10 倍的差距。相反，用户越多，一方面消耗机器资源越多；另一方面，越多的人抢不到商品，平台的客诉和舆情压力也就越大。当然如果为了满足用户，让所有用户都能参与，秒杀系统也可以通过堆机器扩容来实现，但是成本太高，ROI 不划算，所以我们需要提前对流量进行管控。



![img](https://static001.geekbang.org/resource/image/30/a3/300be2283197b07yyaa538e20f175aa3.jpg?wh=2209x1079)



预约期内，开放用户预约，获取秒杀抢购资格；秒杀期内，具备抢购资格的用户真正开始秒杀。在预约期内，关键是锁定用户，这也是我们能够用来做流量管控的核心。在展开通过预约进行流量管控的细节之前，我们先看下如何来设计一个简单的预约系统。

![img](https://static001.geekbang.org/resource/image/b3/90/b33b7faf17887dc8ac4c534bcf92d690.jpg?wh=1584x1430)

传统的预约模式，预约期是固定的时间段，用户在这个阶段内都可以预约；但在秒杀场景下，为了能够准确把控流量，控制预约人数上限，我们需要拓展预约期的定义，除了时间维度外，还要加入预约人数上限的维度，一旦达到上限，预约期就即时结束。这实际上是给预约活动添加了一个自动熔断的功能，一旦活动太火爆，到达上限后系统自动关闭预约入口，提前进入等待秒杀状态。这样就可以准确把控人数，从而为秒杀期护航

![img](https://static001.geekbang.org/resource/image/3d/a4/3d12611a9166fd13bb112038f916e8a4.jpg?wh=1202x1538)





## 六、秒杀的削峰和限流

### 消峰

削峰的方法有很多，可以通过业务手段来削峰，比如秒杀流程中设置验证码或者问答题环节；也可以通过技术手段削峰，比如采用消息队列异步化用户请求，或者采用限流漏斗对流量进行层层过滤。削峰又分为无损和有损削峰。本质上，**限流是一种有损技术削峰**；而引入**验证码、问答题以及异步化消息队列**可以归为无损削峰。

![img](https://static001.geekbang.org/resource/image/35/64/35dc492a82da8e4bbc0188918dcc7964.png?wh=2464x1740)





### 限流

削峰的方式，前面介绍了验证码 / 问答题以及消息队列，这些方式使流量峰值变得更加平滑，但也在一定程度上降低了抢购体验，容易引发用户咨询和投诉。那有没有更好的解决方式呢？接下来我们学习下限流，看看如何通过限流实现削峰



限流是系统自我保护的最直接手段，再厉害的系统，总有所能承载的能力上限，一旦流量突破这个上限，就会引起实例宕机，进而发生系统雪崩，带来灾难性后果。

那么对于秒杀的瞬时流量，如果不加筛选，不做限制，直接把流量传递给下游各个系统，对整个交易系统都是非常大的挑战，也是很大的资源浪费，所以主流的做法是从上游开始，对流量进行逐级限流，分层过滤，优质的有效的流量最终才能参与下单

![img](https://static001.geekbang.org/resource/image/e8/cd/e8e9d01c9e384c8eb7ee6bc908901dcd.jpg?wh=4247x2227)

是系统的流量漏斗示意图，通过风控和防刷筛选刷子流量，通过限购和预约校验过滤无效流量，通过限流丢弃多余流量，最终秒杀系统给到下游的流量就是非常优质且少量的了。

限流常用的算法有令牌桶和漏桶，有关这两个算法的专业介绍，你可以参考：https://hansliu.com/posts/2020/11/what-is-token-bucket-and-leaky-bucket-algorithms.html

![img](https://static001.geekbang.org/resource/image/fb/69/fb564594bb1bb523dde77b678822c269.jpg?wh=2028x844)



#### Nginx 限流

这里的 Nginx 限流，主要是依赖 Nginx 自带的限流功能，针对请求的来源 IP 或者自定义的一个关键参数来做限流，比如用户 ID。其配置限流规则的语法为：
```
limit_req_zone <变量名> zone=<限流规则名称>:<内存大小> rate=<速率阈值>r/s;
```

解释一下：

- 以上 limit_req_zone 是关键字，< 变量名 > 是指定根据什么来限流；
- zone 是关键字，< 限流规则名称 > 是定义规则名称，后续代码中可以指定使用哪个规则；
- < 内存大小 > 是指声明多大内存来支撑限流的功能；
- rate 是关键字，可以指定限流的阈值，单位 r/s 意为每秒允许通过的请求，这个算法是使用令牌漏桶的思想来实现的



那么明白了语法之后，下面我们就动手定义一个限流规则，看看实际效果。

```
http {
    limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s; 
    server {
        location /search/ {
            limit_req zone=one burst=2 nodelay;
        }
    }
}  
```



#### web 应用层限流

以上是 Nginx 网关层的限流，接下来我们进入应用层的限流。应用层的限流手段也是比较多的，这里我们重点介绍通过线程池和 API 限流的方法。

##### 线程池限流

Java 原生的线程池原理相信你非常清楚，我们可以通过自定义线程池，配置最大连接数，以请求处理队列长度以及拒绝策略等参数来达到限流的目的。当处理队列满，而且最大线程都在处理时，多余的请求就会被拒绝策略丢弃，也就是被限流了。

![img](https://static001.geekbang.org/resource/image/7f/56/7f30154bdbc6d9f085dc92bde0216856.jpg?wh=1385x786)

##### API 限流

上面介绍的线程池限流可以看做是一种并发数限流，对于并发数限流来说，实际上服务提供的 QPS 能力是和后端处理的响应时长有关系的，在并发数恒定的情况下，TP99 越低，QPS 就越高。

然而大部分情况是，我们希望根据 QPS 多少来进行限流，这时就不能用线程池策略了。不过，我们可以用 Google 提供的 RateLimiter 开源包，自己手写一个基于令牌桶的限流注解和实现，在业务 API 代码里使用。当然了，大厂中都会有通用的限流机制，你直接用就行了
```
/**  
 * 自定义注解  限流  
 */  

@Target({ElementType.PARAMETER, ElementType.METHOD})  
@Retention(RetentionPolicy.RUNTIME)  
@Documented  
public @interface MyRateLimit {  
     String description() default "";  
}
```
我们自定义一个切面：

```
/**  
 * 限流 AOP  
 */  

@Component  
@Scope  
@Aspect  
public class LimitAspect {  
    //引用RateLimiter，内部是基于令牌桶实现的  
    private static RateLimiter rateLimiter = RateLimiter.create(100.0);  

    //定义限流注解的pointcut  
    @Pointcut("@annotation(com.ecommerce.seckill.aop.MyRateLimit)")    
    public void MyRateLimitAspect() {  
    }  
    
    @Around("MyRateLimitAspect()")  
    public  Object around(ProceedingJoinPoint joinPoint) {   
        Boolean flag = rateLimiter.tryAcquire();  
        Object obj = null;  
        try {  
            if(flag){  
                obj = joinPoint.proceed();  
            }  
        } catch (Throwable e) {  
            e.printStackTrace();  
        }  
        return obj;  
    }  
}
```

业务层代码实现：
```
@Override  
@MyRateLimit  
@Transactional  
public SeckillResponse initData(String skuId, String userName) {  
    //此次为业务代码实现  
}
```

小结这节

我们介绍了削峰的多种手段，有验证码、问答题、消息队列以及限流等。实际上，这些削峰的方式都可以达到控制流量的目的，你可以根据自己的情况进行选择。





## 七、降级、热点和容灾处理

你在手写秒杀系统的时候，可以采用验证码 / 问答题、异步消息队列或者限流的方式进行削峰，以此平滑流量峰值，减轻单位时间分片内的系统压力。这节课我们将把重点放在其他高可用的方面——降级、热点数据和容灾，持续打造秒杀系统的高可用

当秒杀活动开启，流量洪峰来临时，交易系统压力陡增，具体表现一般会包括 CPU 升高，IO 等待变长，请求响应时间 TP99 指标变差，整个系统变得越来越不稳定。为了力保核心交易流程，我们需要对非核心的一些服务进行降级，减轻系统负担，这种降级一般是有损的，属于“弃卒保帅”。

**而秒杀的核心问题，是要解决单个商品的高并发读和高并发写的问题**，这是典型的热点数据问题，我们需要有相应的机制，避免热点数据打垮系统。



### 降级

我们先说说“降级”，其实和削峰一样，降级解决的也是有限的机器资源和超大的流量需求之间的矛盾。如果你的资源够多，或者你的流量不够大，就不需要对系统进行降级了；只有当资源和流量的矛盾突出时，我们才需要考虑系统的降级。

前面已经介绍了，降级一般是有损的，那么必然要有所牺牲，下面介绍几种常见的降级：

- 写服务降级，牺牲数据一致性获取更高的性能；
- 读服务降级，故障场景下紧急降级快速止损；
- 简化系统功能，干掉一些不必要的流程，舍弃非核心功能。



下面我们逐一分析下。



#### 写服务降级，牺牲数据一致性获取更高的性能

我们知道，在多数据源（MySQL 和 Redis）的场景下，数据一致性一般是很难保证的。除非你引入分布式事务，但分布式事务也会带来一些缺点，比如实现复杂、性能问题、可靠性问题等。因此一般在涉及金融资产类对一致性要求高的场景时，我们才会考虑分布式事务。

在流量不高的时候，我们的写请求可以直接先落入 MySQL 数据库，再通过监听数据库的 Binlog 变化，把数据更新进 Redis 缓存，如下图所示。

![img](https://static001.geekbang.org/resource/image/b7/f2/b7551fa007c9bd02cba5fe63c665a7f2.jpg?wh=1200x600)

这种设计，缓存和数据库是最终一致的。通过缓存，我们可以扛更高流量的读操作，但是写操作仍然受制于数据库的磁盘 IOPS，一般考虑一个数据库也就能支持 3000～5000 TPS 的写操作。

当流量激增的时候，我们就需要对以上的写路径进行降级，由同步写数据库降级成同步写缓存、异步写数据库，利用 Redis 强大的 OPS 来扛流量，一般单个 Redis 分片可达 8～10 万的 OPS，Redis 集群的 OPS 就更高了。

如下图所示，写请求首先直接写入 Redis 缓存，写入成功之后，同时再启动一个线程，发出写操作 MQ，就可以返回客户端了。其他应用消费 MQ，通过 MQ 异步化写数据库。

![img](https://static001.geekbang.org/resource/image/8a/49/8a036583d50024606372f1c1d5930c49.jpg?wh=1200x600)

这里，我们通过 Redis 的高并发写能力，提升了系统性能，带来的牺牲就是缓存数据和数据库数据的一致性问题。为了追求高性能，牺牲一致性在大厂的设计中比较常见，对于异步造成的数据丢失等一致性问题，一般会有定时任务一直在比对，以便最快发现问题，进行修复。

#### 读服务降级，故障场景下紧急降级快速止损

在做高可用系统设计时，我们都会有个共识，就是微服务自身所依赖的外部中间件服务或者其他 RPC 服务，随时都可能发生故障，因此我们需要建设多级缓存，以便故障时能及时降级止损。

如下图所示，我们给 Redis 缓存之外，又增加了 ES 缓存。当然了，你可以建立多个缓存副本，比如主 Redis 缓存外，再建立副 Redis 缓存，或者再增加 ES 缓存，这些都是可以的，不过相应会增加你的资源成本和代码编写的复杂度。

![img](https://static001.geekbang.org/resource/image/bc/5f/bc636e43feaf3d44c0364b4b4878455f.jpg?wh=1200x600)

如上图，假设当秒杀的 Redis 缓存出现故障时，我们就可以通过降级开关，快速将读请求降级到 ES 上。或者当 Redis 和 ES 同时出现故障时（现实中很少出现同时故障的场景），我们还是可以通过降级开关将流量切换到数据库上，让数据库暂时承压来完成读请求服务。

由此可见，在做高可用系统设计时，降级路径是多么的重要，它会是你关键时候的保命开关，让你在突发故障时有路可退。



#### 简化系统功能，干掉一些不必要的流程，舍弃非核心功能

当你打开京东或淘宝的商品详情页时，你会发现，除了商品的基本信息外，还有很多附加的信息，比如你是否收藏过该商品、商品的收藏总数量、商品的排行榜、评价和推荐等楼层。同样，对于秒杀结算页，还会有礼品卡、优惠券等虚拟支付路径。

如果是普通商品，这些附加信息当然是越多越好，一方面体现了系统的完整性，另一方面也可以多渠道引流促进转化。但是在秒杀场景下，这些信息是否有必要就需要视情况而定了，**秒杀系统要求尽量简单，交互越少，数据越小，链路越短，离用户越近，响应就越快**，因此非核心的功能在秒杀场景下都是可以降级的，如下图红框所示。

![img](https://static001.geekbang.org/resource/image/23/ee/23acce5a60a381c535d93d1d5a489aee.png?wh=1562x754)

这种非核心功能的有损降级，要视具体的 SKU 而定，一般为了降低影响范围，我们只对流量非常高的 SKU 进行降级。比如，如果是手机秒杀，一般是不需要降级的，但是像茅台、口罩这样的爆品，就需要针对 SKU 维度进行非核心功能的降级了。



这里我们顺便也看下降级开关的设计，比较简单，核心思路就是通过配置中心，对降级开关进行变更，然后推送到各个微服务实例上。

![img](https://static001.geekbang.org/resource/image/c3/33/c3efa7e5f5c4a90791dfe492yyc0b433.jpg?wh=1200x600)

### 热点数据

讲完了降级，接下来我们来聊聊热点数据。进入正题前，我们先看看高并发的常规解决思路。

分布式系统设计，解决高并发问题，可能你很快会想到，如果是数据库，可以通过分库分表来应对，如果是 Redis，可以增加 Redis 集群的分片来解决，而应用层一般是无状态的设计。所以从数据库、Redis 缓存到应用服务，都是可以通过增加机器来水平扩展服务能力，解决高并发的问题。

然而，这样就能应对秒杀的挑战了吗？其实还不够，前面我有提到，秒杀的核心问题是**要解决单个商品的高并发读和高并发写问题，也就是要处理好热点数据问题。**

所谓热点数据，是从单个数据被访问的频次角度去看的。单位时间（1s）内，一个数据非常频繁的被访问，就可以称之为热点数据，反之可以归为一般数据或冷数据。那么单位时间内究竟多高的频次才能称为热点数据呢？实际上并没有一个明确的定义，可以根据你自己的系统吞吐能力而定。

平价茅台在进行秒杀时，只有这个 SKU 是热点，所以再怎么进行分库分表，或者增加 Redis 集群的分片数，茅台 SKU 落在的那个分片的能力实际并没有提升，总会触达上限，把 Redis 打挂，最后可能引发缓存击穿、系统雪崩。那我们应该怎么解决这个棘手的热点问题呢？别担心，难不倒我们，请跟我继续往下学习。

我们把这个问题分为两类：读热点问题和写热点问题。下面我们分别展开讨论。

#### 热点读

先看下读热点如何解决，我先抛出解决该问题的思路：

1. 增加热点数据的副本数。
2. 让热点数据离用户越近越好。

![img](https://static001.geekbang.org/resource/image/0f/f7/0fdee3bf8ecf1706cd5d51a4033c27f7.jpg?wh=1261x1441)

以上是秒杀系统的部署结构图，参照解决思路，我们的第一个解决方案，就是**增加 Redis 从的副本数**，然后业务层（Tomcat 集群）轮询查询不同的副本，提高同一数据的 QPS。一般情况下，单个 Redis 从，可提供 8~10 万的查询，所以如果我们增加 12 个副本，就可以提供百万 QPS 的热点查询。

![img](https://static001.geekbang.org/resource/image/d6/a3/d6952799be5641c2212108f82263e3a3.jpg?wh=1001x678)

这个方法能解决热点问题，但成本比较高，如果你的集群分片数比较多，那分片数 * 副本数就是一笔不小的开销。



第二个解决方案，我们把**热点数据再上移**，在 Tomcat 集群做热点数据的本地缓存，也就是让业务层的每个实例里都有份数据副本，读请求数据的时候，无需去 Redis 获取，直接从本地缓存里取。这时候，数据的副本数和 Tomcat 实例一样多，另外请求链路减少了一层，而且也减少了对 Redis 单片 QPS 上限的依赖，具有更高的可靠性和更高的性能。

![img](https://static001.geekbang.org/resource/image/5a/e4/5a4bcd3ab9dcaaa8d3f13b07aed369e4.jpg?wh=1001x678)

这种方式热点数据的副本数随实例的增加而增加，非常容易扩展，扛高流量。不过你要思考一个问题，本地缓存的数据延迟业务是否能够接受？

如果能接受，本地缓存的时候可以设置几分钟？如果对延迟要求比较高，可以设置 1s，这样对 Redis 而言，OPS 的压力直接降低到实例数 / 每秒，就不需要那么多副本了。

本地缓存的实现比较简单，可以用 HashMap、Ehcache，或者 Google 提供的 Guava 组件。



读热点还有一个比较简单粗暴的方法，那就是直接**短路返回**。这么说可能比较抽象，我举个例子，茅台秒杀的时候，这个 SKU 是不支持使用优惠券的，那么优惠券系统在处理的时候，可以根据配置中心的茅台 SKU 编码，直接返回空的券列表，这样基本上不怎么耗资源，效率非常高。当然了，这种方式和具体商品的活动方式有关，不具有通用性，但是在几百万的流量面前，简单有效。



#### 写热点

介绍完读热点，接下来我们看写热点问题。我们先回忆一下，在流量管控里，我们介绍到用户点击“立即预约”的时候，会往“预约人数”这个 Redis key 上进行 ++ 操作，当几百万人同时预约的时候，这个 key 就是热点写操作了。

这个预约总人数有个特点，只是在前端给用户展示用，除此之外，没有其他用途，因此在高并发的场景下，这个人数可以不用那么及时和精确。知道了问题所在，解决方案就在眼前了，我们的思路就是先在 JVM 内存里 ++，延迟提交到 Redis，这样就可以把 Redis 的 OPS 降低几十倍。以下是示意图：

![img](https://static001.geekbang.org/resource/image/a5/ba/a52768c760c1880dc5240660e37013ba.jpg?wh=1024x1140)





写热点还有一个场景就是库存的扣减，这里讲一下基本思路，可以通过**把一个热 key 拆解成多个 key 的方式，避免热点问题**。这种设计涉及到对库存进行再细分，以及子库存挪动，非常复杂，而且边界问题比较多，容易出现少卖或者超卖问题，一般不推荐这种方法。



另一个思路就是对单 SKU 的库存直**接在 Redis 单分片上进行扣减**，实际上，库存系统在秒杀链路的末端，通过我们之前介绍的削峰和限流，真正到库存的流量是有限的，单片的 Redis OPS 能承受得了。然后，我们可以针对单 SKU 的库存扣减进行限流，保证库存单片 Redis 的压力。这样双管齐下，单 SKU 的库存 Redis 扣减压力就是可控的了。



### 容灾

最后我们一起看下容灾，容灾不仅仅是秒杀系统需要考虑的，但凡重要的系统，都要在方案设计时考虑容灾问题。容灾，一般是指搭建多套（两套或以上）相同的系统，当其中一个系统出现故障时，其他系统能快速进行接管，从而持续提供 7*24 不间断业务。

 我们的重点放在“同城双活”的设计上。

同城双活是在同城或相近区域内建立两个机房。同城双机房距离比较近，通信线路质量较好，比较容易实现数据的同步复制，保证高度的数据完整性和数据零丢失。

同城两个机房各承担一部分流量，一般入口流量完全随机，内部 RPC 调用尽量通过就近路由闭环在同机房，相当于两个机房镜像部署了两个独立集群，数据仍然是单点写到主机房数据库，然后实时同步到另外一个机房。

![img](https://static001.geekbang.org/resource/image/7b/c2/7b31b36b54824fefb9457b8b93b152c2.jpg?wh=2024x1156)

如上图所示，就是秒杀系统的“同城双活”方案。从 Nginx 层、Tomcat 层，到 Redis、MySQL 层，我们都做了双中心部署，不管哪一层出现故障，都可以灵活切换。





## 八、黑产对抗——防刷和风控

经过前面对秒杀业务的介绍，你现在应该清楚，秒杀系统之所以流量高，主要是因为一般使用秒杀系统做活动的商品，基本都是稀缺商品。稀缺商品意味着在市场上具有较高的流通价值，那么它的这一特点，必定会引来一群“聪明”的用户，为了利益最大化，通过非正常手段来抢购商品，这种行为群体我们称之为黑产用户

他们确实是聪明的，因为他们总能想出五花八门的抢购方式，有借助物理工具，像“金手指”这种帮忙点击手机抢购按钮的；有通过第三方软件，按时准点帮忙触发 App 内的抢购按钮的；还有的是通过抓取并分析抢购的相关接口，然后自己通过程序来模拟抢购过程的。

黑产的方法也很简单，就是想法设法比别人快，发出的请求比别人多，就像在一个赛道上，给自己制造很多的分身，不仅保证自己比别人快，同时还要把别人挤出赛道，确保自己能够到达终点。

只针对第一个快的特点，其实在活动开始后，进来的流量我们都无法将其定义为非法流量，这个只能借助像风控这种多维度校验，才能将其识别出来，除非它跳步骤。而第二个高频率的特点，同时也是对秒杀系统造成危害最大的一种，我们还是有很多种手段来应对的。

所以我就给你介绍几种比较有效且经过实践的防刷方案，它们**专门针对高频率以及跳步奏的非法手段**。



### 防刷：Nginx 有条件限流

这里呢，我们是根据用户 ID 来做限流防刷的。

![img](https://static001.geekbang.org/resource/image/a0/99/a09ec76448014de730b6c26c9e776a99.png?wh=1920x337)

意为定义了一个名为 limit_by_user 的限流规则，根据用户 ID 来做限流，限流的速率为同一个用户 1 秒内只允许 1 个请求通过，且为该规则申请的内存大小为 10M。



以上通过限流的方式来防刷，是非常简单且直接的一种方式，**这种方式可以有效解决黑产流量对单个接口的高频请求**，但要想防止刷子不经过前置流程直接提单，还需要引入一个流程编排的 Token 机制。

### 防刷：Token 机制

Token 我想你是知道的，一般都是用来做鉴权的。放到秒杀的业务场景就是，对于**有先后顺序的接口调用**，我们要求进入下个接口之前，要在上个接口获得令牌，不然就认定为非法请求。同时这种方式也可以防止多端操作对数据的篡改，如果我们在 Nginx 层做 Token 的生成与校验，可以做到对业务流程主数据的无侵入。



在 Token 机制下，前端与 nginx 项目中的接口交互时序图如下所示：

![img](https://static001.geekbang.org/resource/image/a6/4d/a6b84a677fe6d411319becd71a5d6e4d.jpg?wh=1710x1083)

这个功能在nginx 端通过lua脚本即可实现



![img](https://static001.geekbang.org/resource/image/be/bd/bef17a652c670eacfe5e53940bee08bd.png?wh=1920x1170)

后续请求验证和生成下游token

![img](https://static001.geekbang.org/resource/image/2f/60/2fe31a8064e6ba9ab3702cfaab9e6860.png?wh=1824x1483)

### 防刷：黑名单机制

黑名单机制分为本地黑名单和集群黑名单两种，接下来我们会重点介绍本地黑名单。该机制顾名思义，就是通过黑名单的方式来拦截非法请求的，但我们的核心问题是黑名单从哪里来呢？

总体来说，有两个来源：一个是从外部导入，可以是风控，也可以是别的渠道；而另一个就是自力更生，自己生成自己用。

前面介绍了 Nginx 有条件限流会过滤掉超过阈值的流量，但不能完全拦截，所以索性就不限流，直接全部放进来。然后我们自己实现一套“逮捕机制”，即利用 Lua 的共享缓存功能，去统计 1 秒内这个用户或者 IP 的请求频率，如果达到了我们设定的阈值，我们就认定其为黑产，然后将其放入到本地缓存黑名单。黑名单可以被所有接口共享，这样用户一旦被认定为黑产，其针对所有接口的请求，都将直接被全部拦截，实现刷子流量的 0 通过

![img](https://static001.geekbang.org/resource/image/4e/68/4eba9144f2e22e040eab4e42a7e81f68.png?wh=1616x1483)

本地黑名单机制的优点就是简单、高效。但也正因为基于单机，如果黑产将请求频率控制在 1*Nginx 机器数以内，按请求理想散落的情况下，那么就不会被我们抓到，所以真要想通过频率来严格限制刷子请求，是可以借助 Redis 来实现集群黑名单的。

实现思路和单机的基本一致，就是使用的内存由本地变为了 Redis，当然这也必然会影响接口的响应性能，所以这里我只给出了一个进一步收紧校验的思路，就不做具体教学了，感兴趣的话可以自己尝试下。



### 风控

以上我们介绍了如何通过防刷的手段与黑产用户对抗。而想要更全面地对抗黑产，我们还需要引入另一个重要的机制，那就是风控。

风控在秒杀业务流程中非常重要，但风控的建立却是非常困难的。成熟的风控体系需要建立在大量的数据之上，并且要通过复杂的实际业务场景考验，不断地做智能修正，才能逐步提高风险识别的准确率。

风控的建设过程，其实就是一个不断完善用户画像的过程，而用户画像是建立风控的基础。一个用户画像的基础要素包括手机号、设备号、身份、IP、地址等，一些延展的信息还包括信贷记录、购物记录、履信记录、工作信息、社保信息等等。这些数据的收集，仅仅依靠单平台是无法做到的，这也是为什么风控的建立需要多平台、广业务、深覆盖，因为只有这样，才能够尽可能多地拿到用户数据。

有了这些数据，所谓的风控，其实就是针对某个用户，在不同的业务场景下，检查用户画像中的某些数据，是否触碰了红线，或者是某几项综合数据，是否触碰了红线。而有了完善的用户画像，那些黑产用户，在风控的照妖镜下，自然也就无处遁形了。



## 九、秒杀的库存与限购

对于像秒杀这种大流量、高并发的业务场景，更不适合直接将全部流量打到库存系统，所以这个时候就需要有个系统能够承接大流量，并且只放和商品库存相匹配的请求量到库存系统，而限购就承担这样的角色。**限购之于库存，就像秒杀之于下单，前者都是后者的过滤网和保护伞。 

### 限购

顾名思义，限购的主要功能就是做商品的限制性购买。因为参加秒杀活动的商品都是爆品、稀缺品，所以为了让更多的用户参与进来，并让有限的投放量惠及到更多的人，所以往往会对商品的售卖做限制，一般限制的维度主要包括两方面。

**商品维度限制**：最基本的限制就是商品活动库存的限制，即每次参加秒杀活动的商品投放量。如果再细分，还可以支持针对不同地区做投放的场景，比如我只想在北京、上海、广州、深圳这些一线城市投放，那么就只有收货地址是这些城市的用户才能参与抢购，而且各地区库存量是隔离的，互不影响。

**个人维度限制：**就是以个人维度来做限制，这里不单单指同一用户 ID，还会从同一手机号、同一收货地址、同一设备 IP 等维度来做限制。比如限制同一手机号每天只能下 1 单，每单只能购买 1 件，并且一个月内只能购买 2 件等。个人维度的限购，体现了秒杀的公平性。

有了这些功能支持之后，再做一个热门秒杀活动时，首先会在限购系统中配置活动库存以及各种个人维度的限购策略；然后在用户提单时，走下限购系统，通过限购的请求，再去做真实库存的扣减，这个时候到库存系统的量已经是非常小了

该限购流程如下图所示：

![img](https://static001.geekbang.org/resource/image/a7/b1/a755edd995f37468850dc23338bd53b1.jpg?wh=1234x732)

那么在介绍完限购之后，下面我再来详细说一下上图中活动库存扣减的实现方案。

### 活动库存扣减方案

我们都知道，用户成功购买一个商品，对应的库存就要完成相应的扣减。而库存的扣减主要涉及到两个核心操作，一个是查询商品库存，另一个是在活动库存充足的情况下，做对应数量的扣减。两个操作拆分开来，都是非常简单的操作，但是在高并发场景下，不好的事情就发生了。

![img](https://static001.geekbang.org/resource/image/34/65/3432cfe3769def280f570ab27a550365.jpg?wh=1178x622)

从图中我们可以看到，库存超卖的问题主要是由两个原因引起的，一个是查询和扣减不是原子操作，另一个是并发引起的请求无序。

所以要解决这个问题，我们就得做到库存扣减的原子性和有序性。理想过程应该如下图所示：‘

![img](https://static001.geekbang.org/resource/image/1e/31/1e42d6e7e2bb5efc95d789b71278c331.jpg?wh=1508x1060)

当然理想很美好，那我们该怎么去实现它呢？

你首先可能会想到利用数据库的行锁机制。这种方式的优点是简单安全，但是其性能比较差，无法适用于我们秒杀业务场景，在请求量比较小的业务场景下，是可以考虑的。

既然数据库不行，那能使用分布式锁吗？即通过 Redis 或者 ZooKeeper 来实现一个分布式锁，以商品维度来加锁，在获取到锁的线程中，按顺序去执行商品库存的查询和扣减，这样就同时实现了顺序性和原子性。

其实这个思路是可以的，只是不管通过哪种方式实现的分布式锁，都是有弊端的。以 Redis 的实现来说，仅仅在设置锁的有效期问题上，就让人头大。如果时间太短，那么业务程序还没有执行完，锁就自动释放了，这就失去了锁的作用；而如果时间偏长，一旦在释放锁的过程中出现异常，没能及时地释放，那么所有的业务线程都得阻塞等待直到锁自动失效，这与我们要实现高性能的秒杀系统是相悖的。所以**通过分布式锁的方式可以实现，但不建议使用**。

那还有其他方式吗？有！我们都知道 Redis 本身就是**单线程的，天生就可以支持操作的顺序**性，如果我们能在一次 Redis 的执行中，同时包含查询和扣减两个命令不就好了吗？庆幸的是，Redis 确实能够支持。



Redis 有个功能，是可以执行 Lua 脚本的（我们 Nginx 服务也有用到 Lua 语言，看来 Lua 语言的适用场景还真不少），并且可以保证脚本中的所有逻辑会在一次执行中按顺序完成。而在 Lua 脚本中，又可以调用 Redis 的原生 API，这样就能同时满足顺序性和原子性的要求了。

当然这里的原子性说法可能不是很准确，因为 Lua 脚本并不会自动帮你完成回滚操作，所以如果你的脚本逻辑中包含两步写操作，需要自己去做回滚。好在我们库存扣减的逻辑针对 Redis 的命令就两种，一个读一个写，并且写命令在最后，这样就不存在需要回滚的问题了。



这里能帮我们实现 Redis 执行 Lua 脚本的命令有两个，一个是 EVAL，另一个是 EVALSHA。

```
EVAL script numkeys key [key ...] arg [arg ...]
```

其中 EVAL 是命令，script 是我们 Lua 脚本的字符串形式，numkeys 是我们要传入的参数数量，key 是我们的入参，可以传入多个，arg 是额外的入参。



但这种方式需要每次都传入 Lua 脚本字符串，不仅浪费网络开销，同时 Redis 需要每次重新编译 Lua 脚本，对于我们追求性能极限的系统来说，不是很完美。

所以这里就要说到另一个命令 EVALSHA 了，原生语法如下：
```
EVALSHA sha1 numkeys key [key ...] arg [arg ...]
```
可以看到其语法与 EVAL 类似，不同的是这里传入的不是脚本字符串，而是一个加密串 sha1。这个 sha1 是从哪来的呢？它是通过另一个命令 SCRIPT LOAD 返回的，该命令是预加载脚本用的，语法为：
```
SCRIPT LOAD script
```
这样的话，我们通过预加载命令，将 Lua 脚本先存储在 Redis 中，并返回一个 sha1，下次要执行对应脚本时，只需要传入 sha1 即可执行对应的脚本。这完美地解决了 EVAL 命令存在的弊端，所以我们这里也是基于 EVALSHA 方式来实现的

既然有了思路，也有了方案，那我们开始用代码实现它吧。

首先我们根据以上介绍的库存扣减核心操作，完成核心 Lua 脚本的编写。其主要实现的功能就是查询库存并判断库存是否充足，如果充足，则做相应的扣减操作，脚本内容如下：
```lua
-- 调用Redis的get指令，查询活动库存，其中KEYS[1]为传入的参数1，即库存key
local c_s = redis.call('get', KEYS[1])
-- 判断活动库存是否充足，其中KEYS[2]为传入的参数2，即当前抢购数量
if not c_s or tonumber(c_s) < tonumber(KEYS[2]) then
   return 0
end
-- 如果活动库存充足，则进行扣减操作。其中KEYS[2]为传入的参数2，即当前抢购数量
redis.call('decrby',KEYS[1], KEYS[2])
```
然后我们将 Lua 脚本转成字符串，并添加脚本预加载机制。

预加载可以有多种实现方式，一个是外部预加载好，生成了 sha1 然后配置到配置中心，这样 Java 代码从配置中心拉取最新 sha1 即可。另一种方式是在服务启动时，来完成脚本的预加载，并生成单机全局变量 sha1。我们这里先采取第二种方式，代码结构如下图所示：

![img](https://static001.geekbang.org/resource/image/47/7d/47c9b70d3ee184f6f822cf93a7dd997d.png?wh=849x624)

以上是将 Lua 脚本转成字符串形式，并通过 @PostConstruct 完成脚本的预加载。然后新增 EVALSHA 方法，如下图所示：

![img](https://static001.geekbang.org/resource/image/1a/62/1a1371b717a91yy229c6bfe09afbe762.png?wh=891x522)

方法入参为活动商品库存 key 以及单次抢购数量，并在内部调用 Lua 脚本执行库存扣减操作。看起来是不是很简单？在写完底层核心方法之后，我们只需要在下单之前，调用该方法即可，具体如下图所示：

![img](https://static001.geekbang.org/resource/image/20/76/20a3cd59468619a6b2f55732458f2276.png?wh=879x309)

一切完成后，接下来就让我们来验证一下，是否会出现超卖的情况吧。



## 十、物理机极致优化

咱们逻辑代码层面的优化，基本都已经差不多了。这个时候，再制约系统性能的，往往是些逻辑代码之外的因素了，这些我们可能很少接触，但却非常重要。

所以今天我们将学习一下物理机相关的优化思路，以及部署在物理机上的 Nginx 的配置优化。同时说明一下，以下的优化都是在 Linux 平台下的优化方式，在其他平台的话，部分优化可能不支持。

### CPU 模式的优化

所谓 CPU 模式的调整，就是调整 CPU 的工作频率，使其呈现出不同的性能表现，以满足特定的业务使用场景。我们一般使用的 Linux 系统，也都有多种模式可供选择，像 PowerSave、OnDemand、Interactive、Performance 等，每个模式的调频方式都不同。

因为考虑到秒杀业务的特殊性，并且有时候活动非常的火热，但过段时间可能就降温了些，所以我们采用的模式也不相同。

像大促期间或者某段时间部分商品持续大力度营销，这时的活动非常火热，流量也高，所以我们需要将 CPU 模式调整成 Performance，即高性能模式。这时 CPU 一直处于超频状态，当然这种状态也是比较耗电的，但是为了更好地开展活动，还是需要打开的。

而当活动处于日常化时可以将 CPU 模式切回成 PowerSave 模式，即节能模式，或者是切回系统的默认模式 OnDemand。

### 网卡中断优化

那调完了 CPU，另一个需要优化的点就是网卡中断了。

“中断”是机器硬件与 CPU 交互的一种方式，即硬件告诉 CPU 有事情要处理了。而网卡中断，就是机器网卡告诉 CPU 要处理网络数据了。

前面我们就有说过秒杀的瞬时流量非常高，带来的问题就是一下子会有非常多的网络请求进来。网卡在收到网络信号后，会通知 CPU 来处理，这时如果我们没有调整过相关配置，那么很有可能处理网卡中断的 CPU 都集中在一个核上。

如果这个时候该 CPU 也在承担处理应用进程的任务，那么就有可能出现单核 CPU 飙升的问题，同时网络数据的处理也会受到影响，导致大量 TCP 重传现象的发生。所以这个时候，我们要做的就是合理分配多核 CPU 资源，专门拿出一个核来处理网卡中断



中心思想是，在Nginx 机器上，把网卡中断处理程序绑定到特定的一个cpu上，把nginx服务绑定到特定的几个cpu上，如果存在本级redis绑定另外几个cpu上



### Nginx 配置优化

#### 全局模块配置

#### HTTP 模块配置



